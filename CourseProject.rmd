---
title: "PML-Course-Project"
author: "Lawrence Gulliver"
date: "04/10/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

## Load the data

First we load the data from the provided Cloudfront link, using Readr.

Within the data there are many cells containing the text '#DIV/0!' where there should be a numerical value, but there does not seem to be an explanation for this behaviour. We will assume it is device/software error (clearly attempting to divide by zero) and simply treat it as a missing value, rather than assuming it will be a persistent feature of new datasets generated by these devices.

```{r loaddata, message=FALSE, warning=FALSE, include=FALSE}
library(readr)
har.train <- read_csv(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
    na = c("", "NA", "#DIV/0!")
)
har.validation <- read_csv(
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
    na = c("", "NA", "#DIV/0!")
)
```

## Defining the data

The dataset consists of 159 variables plus the outcome we want to predict, too many to manually select from, so machine learning will be used to define a model for us. However, some of the variables are not suitable for prediction, namely the first five; these provide the row number, user, three timestamp columns.

The sixth and seventh columns seem to refer to sessions, or 'windows' of time in which data is recorded. Each window corresponds to a single outcome, so what seems to be happening is as the user performs one outcome, multiple observations are taken. Fundamentally, this would imply the best method for predicting whether an exercise is valid or not would be to summarise each event into a range of features - i.e. the max, min, and difference of each observation - and predict on that, which seems to be the case for the columns where 'new_window' contains a value 'yes'.

However, the test set corresponds to single observations selected randomly from the training data, so without further manipulation to move the entirety of those windows into the test set, this method is invalid for this project. Similarly, this appears to be a fundamental issue with this briefing as we could use the windows to easily look up the result of the test observations. In the spirit of the exercise, the window will be omitted from this model, and we'll omit rows where new_window is true. This additionally means we can remove many now-empty columns from the dataset.

```{r dataclean}
library(caret)
library(dplyr)
har.train <- filter(har.train, new_window == "no") %>%
    select(-(1:7), -grep("^amp|^avg|^kur|^max|^min|^ske|^std|^var", colnames(har.train)))
har.train$classe <- as.factor(har.train$classe)
set.seed(995)
split <- createDataPartition(har.train$classe, p = 0.8, list = FALSE)[,1]
har.test <- har.train[-split,]
har.train <- har.train[split,]
```

## Training the model

The `caret` package will be used to create a model for this project, handling the heavy lifting for us. Because of the nature of the activity, we can expect that certain logical conditions can easily tell us if the activity was done properly or not - for example, if an angle becomes too steep or too shallow - so a Random Forest method will likely provide a good result.

```{r trainmodel, message=FALSE, warning=FALSE, cache=TRUE}
library(caret)
set.seed(226)
model <- train(
    x = select(har.train, -classe),
    y = har.train$classe,
    methodList = "rf"
)
```

This takes some time, but the result is worth it.

```{r inspectmodel}
model$finalModel$confusion
```

We're given a model with an estimated OOB error rate of 0.55%, which we'll confirm with the test dataset.

```{r crossvalidate}
har.test.p <- predict(model$finalModel, har.test)
cm <- confusionMatrix(har.test.p, har.test$classe)
kable(as.data.frame.matrix(cm$table), caption = paste("Model accuracy:", cm$overall[1]))
```

This out of sample error rate is even better than initially estimated, confirming the model is extremely effective at predicting the category of the exercise being performed for these users.

## Optimisation

```{r optimise}
plot(model$finalModel)
```

We can see from the above plot that 500 trees is far too generous for this exercise. For deployment purposes, reducing the number of trees to 100 or potentially even 50 would not greatly impact the performance of the model, if at all, while allowing the predictions to be made faster and with less processing.

## Conclusion

We have generated a model capable of predicting very accurately on this dataset using a Random Forest method. Looking at the Feature Importance, the majority of provided metrics had a significant impact on the model, which makes sense as it is classifying multiple types of problems in different areas of the exercise.




